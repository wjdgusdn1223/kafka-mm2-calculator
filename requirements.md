# Kafka & MM2 리소스 계산기 요구사항

## 베이스가 되는 지식

좋아요. Kafka(KRaft 모드)와 MirrorMaker2를 Kubernetes 환경에 배포하기 전, 리소스 산정을 위해 어떤 입력 값이 필요한지 정리하고, 해당 입력 값들을 바탕으로 CPU, 메모리, PVC 용량 등을 어떻게 계산할 수 있는지 구체적인 계산식과 그에 대한 신뢰할 수 있는 출처를 조사하겠습니다.

입력 값에는 메시지 처리량, 메시지 크기, 파티션 수, 복제 계수, 보관 기간, 컨슈머 수, MM2 관련 태스크 설정, 메시지 포맷/압축 여부 등이 포함됩니다. 출력 값은 Kafka Controller, Kafka Broker, MirrorMaker2 각각에 대해 pod 수, CPU/Memory request 및 limit, Heap memory, PVC 용량 등을 산출합니다.

계산식은 반드시 실제 사례나 Kafka/MM2 공식 문서, 클라우드 제공업체의 리소스 가이드, 그리고 실 운영 사례 등에서 근거를 확보하고, 납득 가능한 설명과 함께 제공합니다. 웹페이지나 공식 문서에서 제시하는 수치 기반 리소스 추산 가이드를 표 형태로 정리해 포함하겠습니다.


# Kafka 및 MirrorMaker2 리소스 계산 가이드

## 입력 파라미터

Kafka 클러스터와 MirrorMaker2(MM2)의 자원 산정을 위해 사용자가 제공해야 할 주요 입력 항목은 다음과 같습니다.

1. **메시지 처리량** (초당 메시지 수): 클러스터 전체에 유입되는 평균 또는 피크 메시지 수를 의미합니다. 이를 통해 초당 바이트 처리량을 계산할 수 있습니다 (예: `msgs_per_sec * avg_msg_size`). 메시지 처리량은 브로커의 네트워크 및 디스크 부하를 산정하는 기본 요소입니다. Active-Active 양방향 미러링인 경우, 각 클러스터에는 자체 생산자 데이터와 원격 클러스터로부터 미러된 데이터가 모두 유입되므로 **실제 처리량은 단방향 경우의 약 두 배로 고려**해야 합니다.

2. **평균 메시지 크기** (KB): 각 메시지의 평균 크기입니다. 메시지 크기와 처리량을 곱해 초당 데이터량(예: MB/s)을 계산하며, 디스크 및 네트워크 용량 계산에 사용합니다. 예를 들어, 평균 1KB 메시지를 초당 10,000건 처리하면 약 10 MB/s의 데이터율이 발생합니다.

3. **총 파티션 수**: 모든 토픽의 파티션 개수 합계입니다. 양방향 미러링 시 **원본 토픽 수의 두 배**에 해당하는 파티션이 각 클러스터에 존재하게 됩니다. 이 값은 **리더+팔로워 레플리카를 모두 포함한 파티션 총계**로 계산해야 합니다. 예를 들어 원본 토픽 파티션 합계가 1,000이고, 미러링되어 동일한 수의 토픽이 원격에 복제된다면, 클러스터당 총 파티션 수는 약 2,000이 됩니다. 파티션 수는 브로커 메모리 오버헤드와 파일 핸들 개수, 컨트롤러 메타데이터 크기에 영향을 미치는 중요한 요소입니다.

4. **메시지 보관 기간** (일 단위): 데이터가 클러스터에 저장되는 기간으로, 주어진 retention 기간 동안 저장되는 총 데이터량을 계산하는데 필요합니다. 예를 들어 일일 데이터 유입량이 1TB이고 보관 기간이 7일이면 최소 **7TB의 스토리지**가 필요합니다 (레플리카 팩터 및 여유 용량 제외). 보관 기간은 디스크 용량 산정과 직결되며, 보관 정책(retention)을 조정하면 디스크 사용량을 관리할 수 있습니다.

5. **레플리카 팩터**: 각 토픽의 복제 본수입니다 (예: 3). 레플리카 팩터가 `r`이면, 한 메시지가 클러스터 내 `r`개의 브로커에 저장되므로 **저장용량 및 내부복제 트래픽**이 `r`배로 증가합니다. 또한 클러스터의 **유효 처리량 한계** 계산 시 `r`값이 고려되는데, 예를 들어 저장 장치의 최대 처리량이 일정할 경우 클러스터 총 처리량은 브로커 수 대비 `1/r` 비율로 제한됩니다.

6. **컨슈머 그룹 수**: 동시에 동일한 데이터를 구독하는 **독립적인 컨슈머 그룹의 개수**입니다. 예를 들어 서로 다른 서비스가 각각 데이터를 소비하면 그 수만큼 그룹이 존재합니다. 컨슈머 그룹이 많을수록 **브로커의 아웃바운드 트래픽**이 증가합니다. 각 추가 컨슈머 그룹은 모든 메시지를 한 번 더 읽어가기 때문에, 클러스터 총 *출력* 트래픽이 소비자 그룹 수만큼 늘어납니다. MirrorMaker2도 소스 클러스터 입장에서는 **하나의 컨슈머 그룹**처럼 동작하므로, MM2로 인한 부하를 계산에 포함해야 합니다. (사용자가 입력한 컨슈머 수에 MM2를 포함하지 않았다면 내부적으로 1개를 추가 고려).

이상의 입력값들을 기반으로, 아래에서 Kafka **컨트롤러**, **브로커**, **MirrorMaker2** 각각에 필요한 리소스를 계산하는 방법과 근거를 설명합니다.

## Kafka 컨트롤러 리소스 산정

Kafka 4.0의 KRaft 모드에서는 전용 **컨트롤러 노드**들이 클러스터 메타데이터 관리를 담당합니다. 컨트롤러에 필요한 자원은 브로커보다 적지만, **클러스터 안정성을 위해 충분한 자원을 할당**해야 합니다.

* **Pod 수 (컨트롤러 개수)**: KRaft 컨트롤러는 **2n+1의 홀수 개**로 구성하여 과반 투표(quorum)를 형성해야 합니다. 일반적으로 **프로덕션에서는 3대**의 컨트롤러를 권장하며, 클러스터 규모가 매우 크다면 5대까지 둘 수 있습니다. 3개의 컨트롤러는 하나의 활성 리더와 2개의 스탠바이로 동작하며, 하나까지는 장애 허용이 가능합니다. 컨트롤러 수를 늘리면 메타데이터 부하 분산과 가용성이 높아지지만, 불필요하게 많을 경우 오버헤드만 증가하므로 3\~5 범위에서 선택합니다.

* **CPU (요청/제한)**: 컨트롤러는 메타데이터 업데이트(토픽 생성/삭제, 파티션 리더 선출 등)를 처리하므로 CPU 요구량이 브로커만큼 높지는 않습니다. **Confluent 가이드에 따르면 컨트롤러당 4 vCPU** 정도를 권장하며, 이는 일반적인 메타데이터 작업을 커버하는 충분한 사양입니다. 컨트롤러 프로세스도 결국 Kafka의 한 종류이므로 GC 등 JVM 작업을 위해 전용 CPU 코어를 확보하는 것이 좋습니다. 컨트롤러의 CPU 사용량은 주로 **파티션 수**와 **메타데이터 이벤트 빈도**에 비례합니다. 파티션이 매우 많거나 지속적인 리밸런싱/토픽 변경이 있는 경우 CPU 코어 수를 늘리는 것을 고려합니다.

* **메모리 (요청/제한)**: 컨트롤러는 **메타데이터 로그**를 유지하고 Zookeeper를 대체하는 역할을 수행하므로, **4 GB 정도의 메모리**를 권장합니다. Kafka 자체는 힙 메모리를 많이 요구하지 않으며, **JVM 힙 1 GB 정도**면 충분하고 나머지 메모리는 파일 시스템 페이지캐시로 활용됩니다. 예를 들어 4 GB 중 힙을 1 GB로 설정하면, 나머지 ~~3 GB를 OS 페이지 캐시로 활용할 수 있습니다. 컨트롤러의 메타데이터 크기는 파티션 수에 비례하므로, 수만 개 이상의 파티션을 갖는 초대형 클러스터라면 컨트롤러 메모리를 8~~16 GB까지 높여 메타데이터를 캐싱하는 것도 고려합니다. 하지만 일반적인 규모에서는 4 GB로 충분하며, Confluent도 4 GB RAM을 컨트롤러 노드 기준으로 명시하고 있습니다.

* **스토리지 (PVC 용량, pod당)**: 컨트롤러는 **메타데이터 로그**를 전용 디스크에 기록합니다. 이 로그는 클러스터의 토픽/파티션 목록, 구성, ISR 정보 등을 순차적으로 기록한 것으로 용량이 크지는 않지만, **지속성이 매우 중요**합니다. Confluent 권장사항은 **컨트롤러당 64 GB SSD**를 사용하는 것입니다. 64 GB면 수십만 개 이상의 파티션 메타데이터도 충분히 저장할 수 있는 여유 용량입니다. SSD를 권장하는 이유는 메타데이터 로그의 **I/O 지연을 최소화**하고 빠른 스냅샷/복구를 가능케 하기 위함입니다. 컨트롤러 PVC는 주로 메타데이터용이므로, **요청된 용량 = 할당 용량**으로 설정하고, 여유율을 크게 잡지 않아도 됩니다 (단, Kubernetes 특성상 약간 여유를 두고 예: 100 GB로 설정해도 무방).

## Kafka 브로커 리소스 산정

브로커는 **실제 데이터의 저장과 송수신을 담당**하는 Kafka 노드입니다. 브로커 수와 자원은 주어진 처리량과 파티션 수를 만족시킬 수 있도록 계산해야 하며, **디스크, 네트워크, CPU, 메모리 측면의 종합적 고려가 필요**합니다. 아래에서는 브로커의 **개수**, **CPU**, **메모리**, **스토리지**를 산정하는 방법을 설명합니다.

* **Pod 수 (브로커 개수)**: 필요한 브로커 수는 **파티션 수와 처리량 두 가지 관점**에서 결정됩니다. 먼저 파티션 관점으로, **브로커 당 파티션 수**(리더+팔로워 합산)가 너무 많으면 메모리/FD 오버헤드와 장애 복구 지연이 커집니다. 일반적으로 **브로커 1대당 4,000개 이하의 파티션**이 권장되는 소프트 한계입니다. 따라서 브로커 수는 최소 `ceil( (총 파티션 수 × 레플리카 팩터) / 4000 )`로 산정할 수 있습니다. 예를 들어 총 파티션 2,000개에 RF=3이면 총 복제 파티션 6,000개이며, 4000 기준으로 6,000/4,000 = 1.5이므로 **브로커 최소 2대**가 필요합니다. 다음으로 **처리량 관점**을 봅니다. 각 브로커는 클러스터 전체 처리량을 나눠서 받는데, 입력(inbound) 기준으로 **브로커당 약 `t_cluster / N`의 프로듀서 트래픽**을 처리하고, 추가로 **복제(replication) 트래픽** `t_cluster*(r-1)/N`를 수신합니다. 따라서 **브로커당 디스크 쓰기량**은 `(t_cluster * r) / N` (클러스터 총입력 `t_cluster`, 레플리카 `r`, 브로커 `N`)로 계산됩니다. 반면 브로커의 네트워크 출력은 각 메시지가 **모든 소비자 그룹** 및 **팔로워 복제본**으로 전송되므로, **브로커당 출력량**은 대략 `t_cluster / N * (consumer_groups + r - 1)`가 됩니다. 이때 `(r-1)`은 해당 브로커 리더가 팔로워들에게 보내는 내부 복제, `consumer_groups`는 외부 소비자 전달 횟수를 의미합니다. 브로커 한 대가 감당할 수 있는 네트워크 대역이나 디스크 쓰기 한계를 고려하여, **브로커 수 N은 위 산식 결과 각 자원 사용률이 80% 이내**가 되도록 설정해야 합니다. 예컨대 1대 브로커가 최대 200 MB/s의 디스크/네트워크를 처리할 수 있다고 할 때, 클러스터 입력 300 MB/s, RF=3, 소비자 2 그룹이라면 필요한 브로커 수는 `ceil( (300*3) / (0.8*200) ) = 6`대로 산출됩니다 (디스크 한계 기준). **요약하면, 브로커 수는** `max( ceil(total_partitions*r/4000), ceil(required_by_throughput) )`로 결정하며, 최소 3대 (기본 HA 확보) 이상으로 합니다. 너무 큰 브로커보다 적절한 크기의 브로커 여러 대가 운영/장애 복구에 유리합니다.

* **CPU (요청/제한)**: Kafka 브로커는 주로 **디스크 I/O와 네트워크 I/O가 병목**이 되는 경우가 많아 CPU 여유가 비교적 있는 편입니다. 일반적인 메시지 쓰기/읽기 자체는 CPU 소모가 크지 않으며, **CPU 1코어당 수만 건/초 이상의 메시지 처리가 가능**합니다 (메시지 크기에 따라 수십 MB/s 수준). 다만 **TLS 암호화**, **메시지 압축**, **로그 압축(compaction)** 등의 기능을 사용하면 CPU 요구량이 크게 증가할 수 있습니다. 따라서 CPU 산정 시 기본적으로 **브로커당 4\~8 vCPU**를 **request**로 배정하고, 최대 **24 vCPU**까지 스케일업을 고려합니다. Confluent 사례에서는 일반 클러스터 노드로 **24코어 서버**를 흔히 사용한다고 언급합니다. Kubernetes 환경에서는 CPU *request*는 평소 예상 사용량(예: 대부분의 트래픽 시 60% 미만 점유)을 커버하도록, *limit*은 피크 시 일시적 버스트를 허용하되 100%를 넘지 않도록 설정합니다. 예를 들어 한 브로커가 평균 50% CPU를 사용할 것으로 예측된다면 request=4, limit=8 vCPU로 설정해 여유를 두는 방식입니다. **CPU 여유 40% 이상을 유지**하면 (즉 사용률 60% 이하) 장애 시 리더 이동이나 JVM GC 등의 이벤트를 원활히 처리할 수 있습니다. 모니터링을 통해 CPU 사용률이 지속적으로 60%를 넘으면 브로커를 증설하거나 인스턴스 타입을 올리는 식으로 조정하는 것이 권장됩니다.

* **메모리 (요청/제한)**: Kafka 브로커는 **메모리를 주로 페이지 캐시로 활용**하여 디스크 I/O 성능을 높입니다. 권장 힙 크기는 **6 GB 이하**로, Kafka 서버는 힙을 과도하게 크게 잡을 필요가 없습니다. 실제로 Confluent는 32 GB RAM 시스템에서 힙 6 GB로 설정하면 나머지 26 GB가 파일시스템 캐시로 활용되어 최적이라고 합니다. **메모리 요청**은 JVM 힙 + 운영체제 캐시 용도로 충분한 크기로 산정해야 합니다. 간단한 추산으로, **30초 분량의 데이터를 메모리에 버퍼링**할 수 있도록 잡는 방법이 있습니다. 예를 들어 클러스터 쓰기 처리량이 50 MB/s라면 `50 MB/s * 30초 = 1500 MB` 정도를 페이지캐시로 확보하면 최근 30초치 데이터를 메모리에서 바로 읽을 수 있어 소비자 지연을 줄일 수 있습니다. 이 양에 JVM 힙(예: 4~~6 GB)을 더해 브로커 컨테이너의 메모리 **request**를 결정합니다. **limit**는 OS 캐시가 더 활용될 수 있도록 request보다 높게 줄 수 있는데, 예를 들어 request=8 GiB, limit=32~~48 GiB로 설정하면 Kubernetes 상에서는 기본 성능 보장을 8 GiB로 하되 최대 32 GiB까지 캐싱에 사용할 수 있습니다. 다만 컨테이너 메모리 limit을 너무 크게 두면 OOM 시 급격한 종료 위험이 있으므로, 운영 경험에 따라 조정해야 합니다. 요약하면, **브로커당 32\~64 GB 총 메모리**를 권장하며, 이 중 수 GB만 힙으로 쓰고 나머지는 캐시로 쓰는 구성이 일반적입니다. 32 GB 미만 메모리는 Kafka에서는 비효율적이며, 차라리 브로커 수를 늘이는 편이 낫습니다.

* **스토리지 (PVC 용량, pod당)**: 브로커의 디스크 용량은 **일별 데이터량 × 보관기간 × 레플리카 팩터**로 계산합니다. 계산식은 다음과 같습니다:

  $$
  \text{브로커당 필요 용량} = \frac{\text{일별 입력 데이터량} \times \text{보관일수} \times \text{레플리카 팩터}}{\text{브로커 수}}.
  $$

  예를 들어 일별 500 GB를 생산하고 레플리카 팩터 3, 보관 7일이라면 클러스터 전체 저장량은 \$500\text{GB} \times 7 \times 3 = 10.5\text{TB}\$이며, 브로커가 6대라면 대당 약 1.75 TB씩 저장하게 됩니다. **여기에 디스크 여유율**을 고려해야 합니다. Kafka는 디스크 사용이 85%를 넘지 않도록 운영하는 것이 바람직합니다. 85%를 초과하면 디스크 부족으로 인한 오류 위험이 커지고 성능이 저하되므로 CloudWatch 등으로 사용량 85% 임계치에 도달하면 알람을 주어 증설하거나 retention을 줄이는 조치가 권고됩니다. 따라서 계산된 용량에 **약 15% 여유**를 더 잡아 PVC 크기를 할당합니다. 위 예시의 1.75 TB라면 약 2 TB로 반올림하여 요청할 수 있습니다. 또한 **입출력 성능**을 위해 **SSD**나 NVMe를 사용하는 것이 필수적입니다. 클라우드 환경에서 EBS 같은 네트워크 스토리지를 쓴다면 **프로비저닝된 IOPS/처리량**을 고려해야 합니다. AWS gp3볼륨의 경우 기본 125 MB/s, 최대 1000 MB/s까지 확장 가능하므로, 브로커당 디스크 쓰기속도 `(t_cluster * r)/N`이 이 한계를 넘지 않도록 브로커 수나 스토리지 옵션을 결정합니다. 마지막으로 **JBOD vs RAID**: 여러 디스크를 사용할 경우 Kafka는 파티션을 디스크 수에 균등 분산하지만 완벽하지 않을 수 있으므로, **RAID-0/10 구성을 선호**하거나, 파티션/디스크 불균형에 유의해야 합니다. 일반적으로 **브로커당 1\~2TB SSD x 여러 개**를 묶어 쓰며, Confluent 권장사양은 *broker당 12x1TB SSD (JBOD 또는 RAID10)* 정도로 매우 큰 용량도 제시하고 있습니다. 실제 PVC 용량 산정에서는 위의 **공식 결과에 0.85분의 1 (약 1.18배) 곱**하여 여유를 두는 것을 권고합니다. 또한 장애 시를 대비해 **최소 1개 브로커 분량 여유**(예: 3노드 클러스터에서 한 노드 다운 시 나머지 2노드가 데이터를 임시로 더 보유) 등을 감안해 충분한 디스크를 확보하세요.

## MirrorMaker2 리소스 산정

MirrorMaker2 (MM2)는 Kafka Connect 프레임워크를 기반으로 동작하며, **소스 클러스터의 데이터**를 **타겟 클러스터로 복제**하는 역할을 합니다. 양방향 동기화 시 A->B, B->A 두 방향으로 각각 별도의 MM2 인스턴스(커넥트 클러스터)를 운영하게 됩니다. MM2의 자원 산정에서는 \*\*몇 개의 워커(Pod)\*\*를 사용할지, **tasks.max 병렬도**, **CPU/메모리 요구량**을 결정해야 합니다.

* **Pod 수 (MM2 Connect 워커 개수)**: Kafka Connect (MM2) 클러스터는 **고가용성**과 **부하 분산**을 위해 일반적으로 \*\*2개 이상의 워커 노드(pod)\*\*로 구성합니다. Confluent 권장사양에서는 *Connect 노드 2개*를 기본으로 제시하고 있으며, Replicator(MM2)는 Connect와 동일한 스펙으로 2개 노드를 권장합니다. 2개의 워커로 구성하면 한쪽 워커 장애 시에도 복제 작업이 지속되고, 태스크를 두 워커에 나눠 실행할 수 있습니다. 워커 수를 늘리면 tasks를 보다 나눠 가질 수 있지만, 기본적으로 tasks 수에 맞춰 충분한 CPU를 확보하는 편이 더 중요합니다 (자체적으로는 워커 1\~3개로도 수백 파티션 복제가 가능하므로 과도한 노드 증설은 불필요). **요약**: MirrorMaker2는 방향별로 **최소 2개 pod**로 구성하며, 필요 시 작업량에 따라 3개 이상까지 늘릴 수 있습니다.

* **tasks.max (태스크 병렬 수)**: MirrorMaker2에서 **병렬 처리 태스크 수**를 결정하는 설정입니다. **MirrorSourceConnector는 복제할 토픽-파티션마다 하나의 태스크를 생성**하도록 설계되어 있습니다. 따라서 이상적으로는 `tasks.max = 복제 대상 파티션 수`로 설정하면 각 파티션의 복제를 독립 스레드로 처리하게 되어 최상의 병렬 처리가 가능합니다. 예를 들어 100개 파티션을 복제한다면 tasks.max를 100으로 설정하면 각 파티션별로 태스크가 할당됩니다. 하지만 **태스크 수는 가용 CPU 자원에 맞춰 조정**해야 합니다. 일반적인 가이드로 **CPU 코어 1개당 1개의 태스크**가 이상적이며, Aiven의 운영 사례에 따르면 **기본값으로 CPU 1개당 tasks.max 1**을 적용하고 있습니다. 실제로 Aiven의 MM2에서는 클러스터 총 CPU에 맞춰 tasks.max를 자동 산정하는데, 예를 들어 3노드 \* 각 2 CPU = 총 6 vCPU라면 tasks.max를 6으로 정하여 6개의 태스크를 동시에 실행하도록 합니다. 이러한 설정은 **각 태스크(스레드)가 전담할 CPU 코어를 확보**하여 최적의 throughput을 내도록 하는 것입니다. 만약 태스크 수가 코어 수보다 많으면 한 스레드가 여러 파티션을 순차 처리하게 되어 지연이 커지거나 컨텍스트 스위치가 증가할 수 있습니다. 반대로 태스크 수가 너무 적으면 일부 코어가 놀고 태스크당 여러 파티션을 처리하므로 병렬 처리가 제한됩니다. **요약**: `tasks.max`는 \*\*복제할 전체 파티션 수 (또는 필요시 그 이하)\*\*로 설정하되, **가용한 총 vCPU 수와 동일하거나 약간 높게** 설정합니다. 예를 들어 복제 파티션 50개, MM2 워커 2개 \* 각 4 vCPU = 8코어 환경이라면 tasks.max를 8\~10 정도로 설정해 1코어당 1태스크 원칙을 따른다. (양방향 복제 시 A->B용과 B->A용 각각 설정)

* **CPU (요청/제한)**: MirrorMaker2 워커는 내부적으로 **Kafka Connect** JVM 프로세스로 실행되며, 다수의 소비자/프로듀서 쓰레드를 가집니다. **데이터 변환이나 복잡한 처리를 하지 않는 한** MM2는 주로 네트워크/디스크 I/O 바운드이고 CPU 사용률은 높지 않은 편입니다. Confluent 문서에서도 Connect(Replicator)는 "보통 CPU가 바인딩되지 않으며, 빠른 코어보다 **더 많은 코어**를 사용하는 편이 낫다"고 언급합니다. 따라서 MM2 pod당 CPU **request**는 비교적 낮게 (예: 0.5\~1 vCPU) 시작할 수 있지만, **최대 처리량을 위해 코어를 여유 있게 할당**하는 것이 좋습니다. 만약 tasks.max를 크게 설정했다면 해당 pod들이 그만큼의 스레드를 병렬 실행하므로, **pod 전체 vCPU 합계 ≥ tasks.max**를 확보해야 합니다. 예를 들어 tasks.max=10이라면, 2개 워커에 각각 5개 태스크씩 분산될 수 있으므로 각 워커에 최소 5 vCPU는 필요합니다. **CPU limit**는 필요 시 더 쓸 수 있도록 request보다 높게 (예: 2배) 잡아둡니다. 또한 **MM2 자체가 소비자+프로듀서 역할**을 하므로, TLS 암호화나 압축 사용 시 브로커와 마찬가지로 CPU overhead가 증가할 수 있음을 고려합니다. 정리하면, **MM2 워커당 1\~2 vCPU를 기본 요청**으로 하고, 높은 처리량 환경에서는 4 vCPU 이상으로 늘립니다. 특히 대량의 파티션을 병렬 복제하는 경우 **코어 수 = 태스크 수**로 맞추는 것을 권장합니다.

* **메모리 (요청/제한)**: Kafka Connect 기반 애플리케이션은 **JVM 힙 메모리** 설정이 중요합니다. Confluent 권장 스펙에 따르면 Connect/Replicator 노드는 **0.5\~4 GB 정도의 힙 메모리**를 사용하며, 상황에 따라 조절합니다. MirrorMaker2는 메시지를 일시적으로 버퍼링하고 변환하지는 않지만, **소스 클러스터로부터 받은 레코드를 타겟으로 보내는 동안** 약간의 큐잉이 이루어지므로 너무 적은 힙은 피해야 합니다. 일반적으로 **heap 메모리 1\~2 GB** 정도로도 충분하며, 남는 메모리는 OS 캐시나 JVM 오버헤드로 활용됩니다. Kubernetes에서 메모리 *request*는 이 예상 힙 사용량에 맞춰 설정하고 *limit*은 여유 있게 2배 정도 (예: request 1Gi, limit 2Gi) 줄 수 있습니다. 예를 들어 대용량 복제를 할 경우 메모리 부족으로 Throughput이 저하되지 않도록 워커당 2-4 GB를 할당하면 안전합니다. Heap 사이즈는 Connect 실행 시 `-Xmx` 등으로 조정할 수 있으며, MM2의 워크로드(예: 동시에 다룰 파티션 수)가 늘어나면 GC 부하를 줄이기 위해 힙을 늘릴 수 있습니다. 하지만 **과도한 힙은 GC pause를 길게 만들 수 있으므로 4\~8 GB를 넘기지 않는** 것을 권장합니다. 또한 MirrorCheckpointConnector 등 내부 사용 태스크도 약간의 메모리를 쓰므로 여유를 둡니다. **요약**: MM2 워커당 **1\~4 GB 메모리**를 배정하고, 그중 힙은 약 절반\~75% 수준으로 설정합니다. 필요 시 모니터링 지표인 `HeapMemoryAfterGC` 등을 확인하여 GC 후 힙 사용률이 60%를 넘지 않도록 튜닝합니다.

이상이 입력값을 토대로 산출된 Kafka 컨트롤러, 브로커 및 MirrorMaker2의 자원 산정 방법과 근거입니다. 모든 계산식은 **공식 문서와 최신 모범사례를 기반**으로 했으며, 실제 환경에서는 본 가이드로 도출된 값을 시작점으로 삼아 **부하 테스트와 모니터링을 통해 최종 조정**하는 것이 좋습니다. 필요한 자원을 과소산정하면 성능 저하나 장애가 발생할 수 있고, 과대산정하면 비용이 증가하므로, 제시된 근거들을 참고하여 **균형 잡힌 설계**를 하시기 바랍니다.

**참고 자료:** 각 항목의 인용 출처를 함께 첨부하였으며, Apache Kafka와 MirrorMaker2의 공식 문서, Confluent 가이드, AWS Kafka 운영모범, IBM 및 Aiven의 기술 블로그 등의 최신 정보를 활용하였습니다. (모든 인용은 `【출처†라인】` 형식)


## 요구사항

- 위 베이스 지식을 토대로 kafka와 mm2의 필요 리소스 량을 계산해주는 웹페이지를 만드는 것
- 입력 항목에 예를 들어 동기시에는 토픽이나 파티션이 2배가 되니까 그 부분은 이런 식으로 하고 있다는 등 고려 사항이 현재의 입력에서 어떤식으로 고려되고 계산 되고 있는지를 충분하게 언급하여 이해를 도울 것
- 베이스 지식에는 조금 추상적/애매하게 적혀있는 내용이 있으므로 더욱더 보완하여 납득 가능한 툴로 만들기
- 계산한 뒤에는 왜 이런 값이 되었는지에 관한 근거도 같이 표시하여 조금 더 납득할수 있게 할 것
  - 왜 이만큼의 리소스가 필요한지에 대해 납득할 수 있게 안심할 수 있게 하는것이 매우 중요
- 베이스가 되는 지식에 대한 내용도 열람하기 쉽게 별도 메뉴를 추가하여 보기 쉽게 정리해 둘 것
- 웹 페이지의 언어는 **일본어**일것
